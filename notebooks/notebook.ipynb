{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rossmann Store Sales Forecasting\n",
    "## Machine Learning Zoomcamp 2025 - Midterm Project\n",
    "\n",
    "**Objective**: Predict daily sales for Rossmann drugstore chains to optimize inventory management and business planning.\n",
    "\n",
    "**Dataset**: Rossmann Store Sales from Kaggle Competition\n",
    "- Historical sales data from 1,115 stores\n",
    "- Time period: 2013-2015\n",
    "- Features: Store attributes, promotions, holidays, competition data\n",
    "\n",
    "**Business Value**:\n",
    "- Reduce inventory costs through accurate demand forecasting\n",
    "- Optimize staffing levels based on predicted sales volume\n",
    "- Improve customer satisfaction by preventing stockouts\n",
    "- Enable data-driven business planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Time series libraries\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "data_path = Path('../data')\n",
    "\n",
    "# Check if data files exist\n",
    "required_files = ['train.csv', 'test.csv', 'store.csv']\n",
    "missing_files = [f for f in required_files if not (data_path / f).exists()]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"âŒ Missing data files: {missing_files}\")\n",
    "    print(\"Please run: python ../data/download_data.py\")\n",
    "else:\n",
    "    # Load datasets\n",
    "    print(\"ðŸ“Š Loading datasets...\")\n",
    "    \n",
    "    train_df = pd.read_csv(data_path / 'train.csv')\n",
    "    test_df = pd.read_csv(data_path / 'test.csv')\n",
    "    store_df = pd.read_csv(data_path / 'store.csv')\n",
    "    \n",
    "    print(f\"âœ… Train data: {train_df.shape}\")\n",
    "    print(f\"âœ… Test data: {test_df.shape}\")\n",
    "    print(f\"âœ… Store data: {store_df.shape}\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(\"\\nðŸ“ˆ Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of each dataset\n",
    "print(\"ðŸ” TRAIN DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "print(f\"Memory usage: {train_df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nColumns:\")\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"First 5 rows:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸª STORE DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {store_df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "print(store_df.dtypes)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "display(store_df.head())\n",
    "\n",
    "# Check unique values for categorical columns\n",
    "print(\"\\nðŸ“Š Categorical Variables Summary:\")\n",
    "categorical_cols = ['StoreType', 'Assortment', 'PromoInterval']\n",
    "for col in categorical_cols:\n",
    "    if col in store_df.columns:\n",
    "        print(f\"{col}: {store_df[col].nunique()} unique values\")\n",
    "        print(f\"  Values: {store_df[col].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, name):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    \n",
    "    print(f\"ðŸ” DATA QUALITY ASSESSMENT - {name.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Missing values\n",
    "    missing_vals = df.isnull().sum()\n",
    "    missing_pct = (missing_vals / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_vals,\n",
    "        'Missing %': missing_pct\n",
    "    }).sort_values('Missing Count', ascending=False)\n",
    "    \n",
    "    print(\"ðŸ“Š Missing Values Analysis:\")\n",
    "    display(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Data types and memory\n",
    "    print(f\"\\nðŸ’¾ Memory Usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"ðŸ“ Dataset Shape: {df.shape}\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"ðŸ”„ Duplicate Rows: {duplicates}\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Assess each dataset\n",
    "train_quality = assess_data_quality(train_df, 'Train')\n",
    "store_quality = assess_data_quality(store_df, 'Store')\n",
    "test_quality = assess_data_quality(test_df, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train data with store information\n",
    "print(\"ðŸ”— Merging train and store datasets...\")\n",
    "train_full = train_df.merge(store_df, on='Store', how='left')\n",
    "print(f\"âœ… Merged dataset shape: {train_full.shape}\")\n",
    "\n",
    "# Convert date column\n",
    "train_full['Date'] = pd.to_datetime(train_full['Date'])\n",
    "train_full = train_full.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nðŸ“… Date Range:\")\n",
    "print(f\"From: {train_full['Date'].min()}\")\n",
    "print(f\"To: {train_full['Date'].max()}\")\n",
    "print(f\"Total Days: {(train_full['Date'].max() - train_full['Date'].min()).days}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales distribution analysis\n",
    "print(\"ðŸ’° SALES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic statistics\n",
    "sales_stats = train_full['Sales'].describe()\n",
    "print(\"ðŸ“Š Sales Statistics:\")\n",
    "print(sales_stats)\n",
    "\n",
    "# Check for zero sales\n",
    "zero_sales = (train_full['Sales'] == 0).sum()\n",
    "zero_sales_pct = (zero_sales / len(train_full)) * 100\n",
    "print(f\"\\nðŸ” Zero Sales Days: {zero_sales:,} ({zero_sales_pct:.2f}%)\")\n",
    "\n",
    "# Sales when store is open vs closed\n",
    "print(\"\\nðŸª Sales by Store Status:\")\n",
    "store_status = train_full.groupby('Open')['Sales'].agg(['count', 'mean', 'std']).round(2)\n",
    "display(store_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sales distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Sales Distribution Analysis', fontsize=16, y=1.02)\n",
    "\n",
    "# Filter out zero sales for better visualization\n",
    "sales_nonzero = train_full[train_full['Sales'] > 0]['Sales']\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(sales_nonzero, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Sales Distribution (Excluding Zero Sales)')\n",
    "axes[0, 0].set_xlabel('Sales')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot\n",
    "axes[0, 1].boxplot(sales_nonzero)\n",
    "axes[0, 1].set_title('Sales Box Plot')\n",
    "axes[0, 1].set_ylabel('Sales')\n",
    "\n",
    "# Log-scale histogram\n",
    "axes[1, 0].hist(np.log1p(sales_nonzero), bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Log(Sales + 1) Distribution')\n",
    "axes[1, 0].set_xlabel('Log(Sales + 1)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Sales by day of week\n",
    "daily_sales = train_full[train_full['Sales'] > 0].groupby('DayOfWeek')['Sales'].mean()\n",
    "axes[1, 1].bar(range(1, 8), daily_sales, color='coral', alpha=0.7)\n",
    "axes[1, 1].set_title('Average Sales by Day of Week')\n",
    "axes[1, 1].set_xlabel('Day of Week (1=Monday)')\n",
    "axes[1, 1].set_ylabel('Average Sales')\n",
    "axes[1, 1].set_xticks(range(1, 8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Sales range: {sales_nonzero.min():,.0f} - {sales_nonzero.max():,.0f}\")\n",
    "print(f\"ðŸ“Š Sales variance: {sales_nonzero.var():,.0f}\")\n",
    "print(f\"ðŸ“ Sales std: {sales_nonzero.std():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "print(\"ðŸ“… TIME SERIES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create time-based features\n",
    "train_full['Year'] = train_full['Date'].dt.year\n",
    "train_full['Month'] = train_full['Date'].dt.month\n",
    "train_full['Day'] = train_full['Date'].dt.day\n",
    "train_full['WeekOfYear'] = train_full['Date'].dt.isocalendar().week\n",
    "train_full['Quarter'] = train_full['Date'].dt.quarter\n",
    "\n",
    "# Aggregate daily sales across all stores\n",
    "daily_sales = train_full[train_full['Open'] == 1].groupby('Date')['Sales'].agg(['sum', 'mean', 'count'])\n",
    "daily_sales.columns = ['Total_Sales', 'Avg_Sales', 'Open_Stores']\n",
    "\n",
    "print(\"ðŸ“Š Daily Sales Summary:\")\n",
    "print(daily_sales.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series visualization\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "fig.suptitle('Time Series Analysis', fontsize=16)\n",
    "\n",
    "# Daily total sales\n",
    "axes[0].plot(daily_sales.index, daily_sales['Total_Sales'], alpha=0.7, color='blue')\n",
    "axes[0].set_title('Daily Total Sales Across All Stores')\n",
    "axes[0].set_ylabel('Total Sales')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Weekly seasonality\n",
    "weekly_sales = train_full[train_full['Open'] == 1].groupby('DayOfWeek')['Sales'].mean()\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[1].bar(range(7), weekly_sales, color='orange', alpha=0.7)\n",
    "axes[1].set_title('Average Sales by Day of Week')\n",
    "axes[1].set_ylabel('Average Sales')\n",
    "axes[1].set_xticks(range(7))\n",
    "axes[1].set_xticklabels(days)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly trends\n",
    "monthly_sales = train_full[train_full['Open'] == 1].groupby('Month')['Sales'].mean()\n",
    "axes[2].plot(monthly_sales.index, monthly_sales.values, marker='o', linewidth=2, color='green')\n",
    "axes[2].set_title('Average Sales by Month')\n",
    "axes[2].set_ylabel('Average Sales')\n",
    "axes[2].set_xlabel('Month')\n",
    "axes[2].set_xticks(range(1, 13))\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print insights\n",
    "print(\"\\nðŸ” TIME SERIES INSIGHTS:\")\n",
    "print(f\"ðŸ“… Best day of week: {days[weekly_sales.idxmax() - 1]} (Sales: {weekly_sales.max():,.0f})\")\n",
    "print(f\"ðŸ“… Worst day of week: {days[weekly_sales.idxmin() - 1]} (Sales: {weekly_sales.min():,.0f})\")\n",
    "print(f\"ðŸ“… Best month: {monthly_sales.idxmax()} (Sales: {monthly_sales.max():,.0f})\")\n",
    "print(f\"ðŸ“… Worst month: {monthly_sales.idxmin()} (Sales: {monthly_sales.min():,.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Store Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store performance analysis\n",
    "print(\"ðŸª STORE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Store-level aggregation\n",
    "store_stats = train_full[train_full['Open'] == 1].groupby('Store').agg({\n",
    "    'Sales': ['mean', 'std', 'sum', 'count'],\n",
    "    'Customers': ['mean', 'sum'],\n",
    "    'Promo': ['mean']\n",
    "}).round(2)\n",
    "\n",
    "store_stats.columns = ['Avg_Sales', 'Sales_Std', 'Total_Sales', 'Open_Days', \n",
    "                      'Avg_Customers', 'Total_Customers', 'Promo_Rate']\n",
    "\n",
    "# Add store information\n",
    "store_analysis = store_stats.merge(store_df, on='Store', how='left')\n",
    "\n",
    "print(\"ðŸ“Š Store Performance Summary:\")\n",
    "print(store_analysis.describe().round(2))\n",
    "\n",
    "# Top and bottom performing stores\n",
    "print(\"\\nðŸ† TOP 10 STORES (by average sales):\")\n",
    "top_stores = store_analysis.nlargest(10, 'Avg_Sales')[['Avg_Sales', 'StoreType', 'Assortment']]\n",
    "display(top_stores)\n",
    "\n",
    "print(\"\\nâš ï¸ BOTTOM 10 STORES (by average sales):\")\n",
    "bottom_stores = store_analysis.nsmallest(10, 'Avg_Sales')[['Avg_Sales', 'StoreType', 'Assortment']]\n",
    "display(bottom_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for machine learning models\n",
    "print(\"âš™ï¸ FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Create comprehensive feature set for modeling\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
    "    df['Quarter'] = df['Date'].dt.quarter\n",
    "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
    "    \n",
    "    # Cyclical encoding for seasonal patterns\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "    df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "    df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "    \n",
    "    # Competition features\n",
    "    df['CompetitionDistance'].fillna(df['CompetitionDistance'].median(), inplace=True)\n",
    "    df['CompetitionDistance_log'] = np.log1p(df['CompetitionDistance'])\n",
    "    \n",
    "    # Competition open duration\n",
    "    df['CompetitionOpenSinceMonth'].fillna(0, inplace=True)\n",
    "    df['CompetitionOpenSinceYear'].fillna(0, inplace=True)\n",
    "    \n",
    "    df['CompetitionOpen'] = ((df['CompetitionOpenSinceYear'] > 0) & \n",
    "                           (df['CompetitionOpenSinceMonth'] > 0)).astype(int)\n",
    "    \n",
    "    # Promo2 features\n",
    "    df['Promo2SinceWeek'].fillna(0, inplace=True)\n",
    "    df['Promo2SinceYear'].fillna(0, inplace=True)\n",
    "    df['PromoInterval'].fillna('None', inplace=True)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    df['StoreType_encoded'] = df['StoreType'].astype('category').cat.codes\n",
    "    df['Assortment_encoded'] = df['Assortment'].astype('category').cat.codes\n",
    "    \n",
    "    # Holiday interactions\n",
    "    df['Holiday_Promo'] = df['SchoolHoliday'] * df['Promo']\n",
    "    df['StateHoliday_binary'] = (df['StateHoliday'] != '0').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "train_features = create_features(train_full)\n",
    "\n",
    "print(f\"âœ… Original features: {train_full.shape[1]}\")\n",
    "print(f\"âœ… Enhanced features: {train_features.shape[1]}\")\n",
    "print(f\"âœ… New features added: {train_features.shape[1] - train_full.shape[1]}\")\n",
    "\n",
    "# List new features\n",
    "new_features = [col for col in train_features.columns if col not in train_full.columns]\n",
    "print(f\"\\nðŸ†• New features: {new_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "print(\"ðŸ¤– MODEL TRAINING PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Filter open stores and non-zero sales for training\n",
    "model_data = train_features[(train_features['Open'] == 1) & (train_features['Sales'] > 0)].copy()\n",
    "\n",
    "print(f\"ðŸ“Š Model training data: {model_data.shape}\")\n",
    "print(f\"ðŸ“Š Sales range: {model_data['Sales'].min():,.0f} - {model_data['Sales'].max():,.0f}\")\n",
    "\n",
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    'Store', 'DayOfWeek', 'Promo', 'SchoolHoliday',\n",
    "    'Year', 'Month', 'Day', 'WeekOfYear', 'Quarter', 'DayOfYear',\n",
    "    'Month_sin', 'Month_cos', 'DayOfWeek_sin', 'DayOfWeek_cos',\n",
    "    'StoreType_encoded', 'Assortment_encoded',\n",
    "    'CompetitionDistance_log', 'CompetitionOpen',\n",
    "    'Promo2', 'Holiday_Promo', 'StateHoliday_binary'\n",
    "]\n",
    "\n",
    "# Ensure all features exist\n",
    "available_features = [col for col in feature_columns if col in model_data.columns]\n",
    "print(f\"\\nðŸ“‹ Available features: {len(available_features)}\")\n",
    "print(f\"ðŸ“‹ Missing features: {set(feature_columns) - set(available_features)}\")\n",
    "\n",
    "X = model_data[available_features]\n",
    "y = model_data['Sales']\n",
    "\n",
    "print(f\"\\nâœ… Feature matrix: {X.shape}\")\n",
    "print(f\"âœ… Target vector: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Training set: {X_train.shape}\")\n",
    "print(f\"ðŸ“Š Test set: {X_test.shape}\")\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate and display model performance metrics\"\"\"\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'RÂ²': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Store model results\n",
    "model_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linear Regression (Baseline)\n",
    "print(\"ðŸ”µ Training Linear Regression (Baseline)...\")\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "lr_results = evaluate_model(y_test, lr_pred, 'Linear Regression')\n",
    "model_results.append(lr_results)\n",
    "\n",
    "print(\"âœ… Linear Regression completed\")\n",
    "print(f\"   RMSE: {lr_results['RMSE']:.2f}\")\n",
    "print(f\"   RÂ²: {lr_results['RÂ²']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Random Forest\n",
    "print(\"ðŸŒ³ Training Random Forest...\")\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_results = evaluate_model(y_test, rf_pred, 'Random Forest')\n",
    "model_results.append(rf_results)\n",
    "\n",
    "print(\"âœ… Random Forest completed\")\n",
    "print(f\"   RMSE: {rf_results['RMSE']:.2f}\")\n",
    "print(f\"   RÂ²: {rf_results['RÂ²']:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ” Top 10 Most Important Features:\")\n",
    "display(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. XGBoost\n",
    "print(\"ðŸš€ Training XGBoost...\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_results = evaluate_model(y_test, xgb_pred, 'XGBoost')\n",
    "model_results.append(xgb_results)\n",
    "\n",
    "print(\"âœ… XGBoost completed\")\n",
    "print(f\"   RMSE: {xgb_results['RMSE']:.2f}\")\n",
    "print(f\"   RÂ²: {xgb_results['RÂ²']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. Time Series Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prophet Time Series Model\n",
    "print(\"ðŸ“ˆ Training Prophet (Time Series Forecasting)...\")\n",
    "\n",
    "# Aggregate sales by date across all stores for time series analysis\n",
    "prophet_data = train_features[\n",
    "    (train_features['Open'] == 1) & \n",
    "    (train_features['Sales'] > 0)\n",
    "].groupby('Date').agg({\n",
    "    'Sales': 'sum',\n",
    "    'SchoolHoliday': 'max',\n",
    "    'Promo': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Prepare Prophet format\n",
    "prophet_df = prophet_data[['Date', 'Sales']].copy()\n",
    "prophet_df.columns = ['ds', 'y']\n",
    "prophet_df = prophet_df.sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "# Add regressors\n",
    "prophet_df['school_holiday'] = prophet_data['SchoolHoliday'].values\n",
    "prophet_df['promo_count'] = prophet_data['Promo'].values\n",
    "\n",
    "# Split for time series validation\n",
    "split_date = prophet_df['ds'].quantile(0.8)\n",
    "train_prophet = prophet_df[prophet_df['ds'] <= split_date].copy()\n",
    "test_prophet = prophet_df[prophet_df['ds'] > split_date].copy()\n",
    "\n",
    "print(f\"ðŸ“Š Prophet train data: {len(train_prophet)} days\")\n",
    "print(f\"ðŸ“Š Prophet test data: {len(test_prophet)} days\")\n",
    "\n",
    "try:\n",
    "    # Initialize Prophet with business-specific parameters\n",
    "    prophet_model = Prophet(\n",
    "        daily_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        yearly_seasonality=True,\n",
    "        seasonality_mode='multiplicative',\n",
    "        changepoint_prior_scale=0.1\n",
    "    )\n",
    "    \n",
    "    # Add custom regressors\n",
    "    prophet_model.add_regressor('school_holiday')\n",
    "    prophet_model.add_regressor('promo_count')\n",
    "    \n",
    "    # Fit model\n",
    "    prophet_model.fit(train_prophet)\n",
    "    \n",
    "    # Make predictions\n",
    "    future = prophet_model.make_future_dataframe(periods=len(test_prophet), freq='D')\n",
    "    \n",
    "    # Add regressor values\n",
    "    full_data = pd.concat([train_prophet, test_prophet])\n",
    "    future = future.merge(\n",
    "        full_data[['ds', 'school_holiday', 'promo_count']], \n",
    "        on='ds', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing values\n",
    "    future['school_holiday'].fillna(0, inplace=True)\n",
    "    future['promo_count'].fillna(future['promo_count'].median(), inplace=True)\n",
    "    \n",
    "    forecast = prophet_model.predict(future)\n",
    "    \n",
    "    # Extract test predictions\n",
    "    test_predictions = forecast[forecast['ds'].isin(test_prophet['ds'])]['yhat'].values\n",
    "    test_actual = test_prophet['y'].values\n",
    "    \n",
    "    # Evaluate\n",
    "    prophet_results = evaluate_model(test_actual, test_predictions, 'Prophet')\n",
    "    model_results.append(prophet_results)\n",
    "    \n",
    "    print(\"âœ… Prophet completed\")\n",
    "    print(f\"   RMSE: {prophet_results['RMSE']:.2f}\")\n",
    "    print(f\"   RÂ²: {prophet_results['RÂ²']:.4f}\")\n",
    "    print(\"   ðŸ“Š Prophet excels at capturing seasonality and holiday effects\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Prophet training failed: {str(e)}\")\n",
    "    print(\"âš ï¸ Skipping Prophet model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ARIMA Time Series Model\n",
    "print(\"\\nðŸ“Š Training ARIMA (Classical Time Series)...\")\n",
    "\n",
    "try:\n",
    "    # Aggregate daily sales for ARIMA\n",
    "    arima_data = train_features[\n",
    "        (train_features['Open'] == 1) & \n",
    "        (train_features['Sales'] > 0)\n",
    "    ].groupby('Date')['Sales'].sum().reset_index()\n",
    "    \n",
    "    arima_data = arima_data.sort_values('Date').reset_index(drop=True)\n",
    "    arima_data.set_index('Date', inplace=True)\n",
    "    \n",
    "    print(f\"ðŸ“Š ARIMA data points: {len(arima_data)}\")\n",
    "    \n",
    "    # Check stationarity\n",
    "    adf_result = adfuller(arima_data['Sales'])\n",
    "    is_stationary = adf_result[1] <= 0.05\n",
    "    print(f\"ðŸ“Š Stationarity test p-value: {adf_result[1]:.4f}\")\n",
    "    print(f\"ðŸ“Š Series is {'stationary' if is_stationary else 'non-stationary'}\")\n",
    "    \n",
    "    # Split for time series validation\n",
    "    split_idx = int(len(arima_data) * 0.8)\n",
    "    train_arima = arima_data.iloc[:split_idx]\n",
    "    test_arima = arima_data.iloc[split_idx:]\n",
    "    \n",
    "    # ARIMA parameters\n",
    "    order = (1, 0 if is_stationary else 1, 1)\n",
    "    print(f\"ðŸ“Š Using ARIMA order: {order}\")\n",
    "    \n",
    "    # Fit ARIMA model\n",
    "    arima_model = ARIMA(train_arima['Sales'], order=order)\n",
    "    arima_fitted = arima_model.fit()\n",
    "    \n",
    "    # Make predictions\n",
    "    n_periods = len(test_arima)\n",
    "    arima_forecast = arima_fitted.forecast(steps=n_periods)\n",
    "    \n",
    "    # Evaluate\n",
    "    arima_results = evaluate_model(\n",
    "        test_arima['Sales'].values, \n",
    "        arima_forecast, \n",
    "        'ARIMA'\n",
    "    )\n",
    "    model_results.append(arima_results)\n",
    "    \n",
    "    print(\"âœ… ARIMA completed\")\n",
    "    print(f\"   RMSE: {arima_results['RMSE']:.2f}\")\n",
    "    print(f\"   RÂ²: {arima_results['RÂ²']:.4f}\")\n",
    "    print(\"   ðŸ“Š ARIMA provides statistical time series foundation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ARIMA training failed: {str(e)}\")\n",
    "    print(\"âš ï¸ Skipping ARIMA model\")\n",
    "    print(\"ðŸ’¡ Tip: Install statsmodels with: pip install statsmodels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"ðŸ“Š MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.round(4)\n",
    "display(results_df)\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = results_df['RÂ²'].idxmax()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "\n",
    "print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
    "print(f\"   RÂ² Score: {results_df.loc[best_model_idx, 'RÂ²']:.4f}\")\n",
    "print(f\"   RMSE: {results_df.loc[best_model_idx, 'RMSE']:.2f}\")\n",
    "print(f\"   MAPE: {results_df.loc[best_model_idx, 'MAPE']:.2f}%\")\n",
    "\n",
    "# Select best model for deployment\n",
    "if best_model_name == 'Linear Regression':\n",
    "    best_model = lr_model\n",
    "    best_predictions = lr_pred\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "    best_predictions = rf_pred\n",
    "else:\n",
    "    best_model = xgb_model\n",
    "    best_predictions = xgb_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Validation and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle(f'Model Validation - {best_model_name}', fontsize=16)\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0, 0].scatter(y_test, best_predictions, alpha=0.5)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Sales')\n",
    "axes[0, 0].set_ylabel('Predicted Sales')\n",
    "axes[0, 0].set_title('Actual vs Predicted')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - best_predictions\n",
    "axes[0, 1].scatter(best_predictions, residuals, alpha=0.5)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted Sales')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residual Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals histogram\n",
    "axes[1, 0].hist(residuals, bins=50, alpha=0.7, color='orange')\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Residuals Distribution')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model comparison\n",
    "models = results_df['Model']\n",
    "r2_scores = results_df['RÂ²']\n",
    "axes[1, 1].bar(models, r2_scores, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "axes[1, 1].set_ylabel('RÂ² Score')\n",
    "axes[1, 1].set_title('Model Performance Comparison')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print validation insights\n",
    "print(\"\\nðŸ” MODEL VALIDATION INSIGHTS:\")\n",
    "print(f\"ðŸ“Š Mean residual: {residuals.mean():.2f}\")\n",
    "print(f\"ðŸ“Š Std residual: {residuals.std():.2f}\")\n",
    "print(f\"ðŸ“Š Max absolute error: {np.abs(residuals).max():,.0f}\")\n",
    "print(f\"ðŸ“Š Median absolute error: {np.abs(residuals).median():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and preprocessing artifacts\n",
    "print(\"ðŸ’¾ SAVING MODEL ARTIFACTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "import os\n",
    "model_dir = Path('../src/models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "model_path = model_dir / 'best_model.joblib'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"âœ… Model saved: {model_path}\")\n",
    "\n",
    "# Save feature list\n",
    "feature_list_path = model_dir / 'feature_list.joblib'\n",
    "joblib.dump(available_features, feature_list_path)\n",
    "print(f\"âœ… Feature list saved: {feature_list_path}\")\n",
    "\n",
    "# Save store information for predictions\n",
    "store_info_path = model_dir / 'store_info.joblib'\n",
    "joblib.dump(store_df, store_info_path)\n",
    "print(f\"âœ… Store info saved: {store_info_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': type(best_model).__name__,\n",
    "    'features': available_features,\n",
    "    'performance': results_df.loc[best_model_idx].to_dict(),\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test)\n",
    "}\n",
    "\n",
    "metadata_path = model_dir / 'model_metadata.joblib'\n",
    "joblib.dump(model_metadata, metadata_path)\n",
    "print(f\"âœ… Model metadata saved: {metadata_path}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All model artifacts saved successfully!\")\n",
    "print(f\"ðŸ“ Model directory: {model_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project summary\n",
    "print(\"ðŸ“‹ PROJECT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ðŸŽ¯ Project: Rossmann Sales Forecasting\")\n",
    "print(f\"ðŸ“Š Dataset: {train_full.shape[0]:,} records, {len(store_df)} stores\")\n",
    "print(f\"ðŸ“… Time period: {train_full['Date'].min()} to {train_full['Date'].max()}\")\n",
    "print(f\"ðŸ† Best model: {best_model_name}\")\n",
    "print(f\"ðŸ“ˆ Model performance: RÂ² = {results_df.loc[best_model_idx, 'RÂ²']:.4f}\")\n",
    "print(f\"ðŸŽ¯ RMSE: {results_df.loc[best_model_idx, 'RMSE']:.2f}\")\n",
    "print(f\"ðŸŽ¯ MAPE: {results_df.loc[best_model_idx, 'MAPE']:.2f}%\")\n",
    "\n",
    "print(\"\\nâœ… COMPLETED TASKS:\")\n",
    "print(\"   âœ“ Data exploration and quality assessment\")\n",
    "print(\"   âœ“ Comprehensive EDA with time series analysis\")\n",
    "print(\"   âœ“ Feature engineering with time-based features\")\n",
    "print(\"   âœ“ Multiple model training and comparison\")\n",
    "print(\"   âœ“ Model validation and performance evaluation\")\n",
    "print(\"   âœ“ Model artifacts saved for deployment\")\n",
    "\n",
    "print(\"\\nðŸš€ NEXT STEPS:\")\n",
    "print(\"   1. Export training logic to train.py script\")\n",
    "print(\"   2. Create Flask web service (predict.py)\")\n",
    "print(\"   3. Build Docker container\")\n",
    "print(\"   4. Deploy to cloud platform\")\n",
    "print(\"   5. Create comprehensive documentation\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Notebook completed successfully!\")\n",
    "print(\"Ready for production deployment phase.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}